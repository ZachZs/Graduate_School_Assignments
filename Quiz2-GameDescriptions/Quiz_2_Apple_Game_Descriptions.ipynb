{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#3c38a8\">Quiz 2 - Apple Strategy Game Reviews</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#3c38a8\">11/09/2019</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy as sp\n",
    "import datetime\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import SparsePCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from imblearn.over_sampling import ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the working directory\n",
    "%cd \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"#3c38a8\">Step 1: Data Cleaning and Dimension Reduction</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create Response Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the excel file with the ratings\n",
    "df_ratings = pd.read_csv(\"apple1000new.csv\")\n",
    "\n",
    "df_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the game name and rating\n",
    "df_ratings = df_ratings.iloc[:,[0,2]]\n",
    "\n",
    "df_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a categorical column of the scores\n",
    "df_ratings[\"rating_cat\"] = pd.cut(df_ratings['Average User Rating'], [0,3.1,4.1,5.1], labels = ['Poor', 'Good', 'Great'])\n",
    "\n",
    "df_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Examination of Feature Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data file and drop the unnecessary index column\n",
    "df_raw = pd.read_csv(\"Apple1000games.csv\")\n",
    "df_raw = df_raw.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if any rows have no words associated with them -> if the max value is 0, then no words appear at all,\n",
    "# and the row could be considered 'missing'\n",
    "df_raw.describe().T.sort_values('max', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the ratings with the raw data -> assumption is that the word data file is in the same order as the information\n",
    "# file about the games\n",
    "df_raw['rating_cat'] = df_ratings[\"rating_cat\"]\n",
    "df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the rows where there is no rating, by selecting the notnull ratings\n",
    "df1 = df_raw[df_raw['rating_cat'].notnull()]\n",
    "\n",
    "# reset the index and don't add the index as a column\n",
    "df1.reset_index(inplace=True, drop=True)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove columns with a \\ in them, using a regex filter (NOTE: \\ is selected using \\\\\\\\)\n",
    "df1 = df1[df1.columns.drop(list(df1.filter(regex='\\\\\\\\')))]\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Standardize Feature Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select all columns except for the ratings, and check the last value to confirm it is not the ratings\n",
    "feature_list = list(df1.columns)[0:-1]\n",
    "feature_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the features, using the previous cell list of feature names\n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(df1[feature_list])\n",
    "df1_scaled = scaler.transform(df1[feature_list])\n",
    "\n",
    "# confirm the shape of the first row\n",
    "df1_scaled[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Dimension Reduction - PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the start time\n",
    "print(\"Start Time:\", datetime.datetime.now().time())\n",
    "\n",
    "# Run regular PCA on the standardized data for the first 90% of variance\n",
    "pca = PCA(0.90)\n",
    "pca.fit(df1_scaled)\n",
    "pca_components = pca.transform(df1_scaled)\n",
    "\n",
    "# create a blank cols list\n",
    "cols = []\n",
    "\n",
    "# loop through a for loop (the number of times is equal to the number of values in the first pca_component, and is\n",
    "# equal to the number of PCs created), and create a col name that is added to the col list\n",
    "for i in range(len(pca_components[1])):\n",
    "    cols.append(\"PC\"+str(i+1))\n",
    "    \n",
    "# get the end time\n",
    "print(\"End Time:\", datetime.datetime.now().time())\n",
    "\n",
    "# put these PCs in a df\n",
    "df_pca = pd.DataFrame(pca_components, columns = cols)\n",
    "df_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Dimension Reduction - Sparse PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the start time\n",
    "print(\"Start Time:\", datetime.datetime.now().time())\n",
    "\n",
    "# Run sparse PCA on the standardized data for 20 components\n",
    "s_pca = SparsePCA(n_components=20)\n",
    "s_pca.fit(df1_scaled)\n",
    "s_pca_components = s_pca.transform(df1_scaled)\n",
    "\n",
    "# create a blank cols list\n",
    "cols = []\n",
    "\n",
    "# loop through a for loop 20 times (the number of components), and create a col name that is added to the col list\n",
    "for i in range(1,21):\n",
    "    cols.append(\"S_PC\"+str(i))\n",
    "\n",
    "# get the end time\n",
    "print(\"End Time:\", datetime.datetime.now().time())    \n",
    "\n",
    "# put these PCs in a df\n",
    "df_s_pca = pd.DataFrame(s_pca_components, columns = cols)\n",
    "df_s_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Dimension Reduction - t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the perplexities of several values\n",
    "\n",
    "# create a list of various perplexities\n",
    "list_of_perplex = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "\n",
    "# create a list of various learning rates\n",
    "list_of_learn = [200, 500, 1000]\n",
    "\n",
    "# loop through all of the learning rates\n",
    "for i in list_of_learn:\n",
    "    \n",
    "    # loop through all the perplexities\n",
    "    for j in list_of_perplex:\n",
    "\n",
    "        # run the t-SNE on the standardized data for 3 dimensions\n",
    "        tsne_components = TSNE(n_components=3, learning_rate=i, perplexity=j).fit_transform(df1_scaled)\n",
    "\n",
    "        # create a blank cols list\n",
    "        cols = []\n",
    "\n",
    "        # loop through a for loop 3 times (the number of components), and create a col name that is added to the \n",
    "        # col list\n",
    "        for k in range(1,4):\n",
    "            cols.append(\"tSNE\"+str(k))\n",
    "    \n",
    "        # put these PCs in a df\n",
    "        df_tsne = pd.DataFrame(tsne_components, columns = cols)\n",
    "        df_tsne\n",
    "    \n",
    "        print(\"Learning Rate:\", str(i))\n",
    "        print(\"Perplexity:\", str(j))\n",
    "    \n",
    "        # create a 3-D scatter plot\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.scatter(df_tsne['tSNE1'], df_tsne['tSNE2'], df_tsne['tSNE3'], c='skyblue', s=60)\n",
    "        ax.view_init(30, 185) # adjust view angle\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"##############################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on the previous investigation of the visualizations of learning rate and perplexity, I chose 500 and 50\n",
    "\n",
    "# get the start time\n",
    "print(\"Start Time:\", datetime.datetime.now().time())\n",
    "\n",
    "# run the t-SNE on the standardized data for 3 dimensions\n",
    "tsne_components = TSNE(n_components=3, perplexity=50, learning_rate=500).fit_transform(df1_scaled)\n",
    "\n",
    "# create a blank cols list\n",
    "cols = []\n",
    "\n",
    "# loop through a for loop 3 times (the number of components), and create a col name that is added to the col list\n",
    "for j in range(1,4):\n",
    "    cols.append(\"tSNE\"+str(j))\n",
    "\n",
    "# get the end time\n",
    "print(\"End Time:\", datetime.datetime.now().time())\n",
    "    \n",
    "# put these PCs in a df\n",
    "df_tsne = pd.DataFrame(tsne_components, columns = cols)\n",
    "df_tsne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Combine Dimension Reduction Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat all three dimension reduction sets into one table\n",
    "df_dr = pd.concat([df_tsne, df_s_pca, df_pca], axis=1)\n",
    "\n",
    "# add the rating column\n",
    "df_dr['rating_cat'] = df1['rating_cat']\n",
    "\n",
    "df_dr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see that ratings columns carried over correctly (missing data means the indexing got messed up somewhere)\n",
    "missing = sum(df_dr['rating_cat'].isnull().values)\n",
    "missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"#3c38a8\">Step 2: Classification</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Define Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the KNN funtion\n",
    "\n",
    "def knn(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    # get the start time\n",
    "    print(\"Start Time:\", datetime.datetime.now().time())\n",
    "    \n",
    "    # list all of the neighbor parameters\n",
    "    k_list = [5, 7, 9, 11, 13, 15]\n",
    "    \n",
    "    # Create a dataframe of the excel template\n",
    "    knn_df = pd.DataFrame(columns=['Your Name',\n",
    "                               'Random State',\n",
    "                               'Algorithm',\n",
    "                               'k-Neighbors',\n",
    "                               'n_estimators',\n",
    "                               'learning_rate',\n",
    "                               'subsample',\n",
    "                               'max_depth',\n",
    "                               'Best F1 Score',\n",
    "                               'Best Accuracy Score'])\n",
    "    \n",
    "    # loop through all of the neighbor parameters\n",
    "    for i in k_list:\n",
    "    \n",
    "        # Create the classifier based on i neighbors and fit it to the data along with correct labels\n",
    "        clf = KNeighborsClassifier(n_neighbors=i, algorithm = 'kd_tree', metric=\"euclidean\", p=2)  \n",
    "        clf.fit(X_train, y_train)\n",
    "            \n",
    "        # create a prediction based on test set\n",
    "        y_pred = clf.predict(X_test)\n",
    "            \n",
    "        # find the F1 score (macro does not take into account class imbalances -> doesn't appear to be \n",
    "        # any in this data)\n",
    "        f_one = f1_score(y_test , y_pred, average='macro')\n",
    "        \n",
    "        # find the accuracy score\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "        temp = pd.DataFrame([['Name', 20, 'KNN', i, 'NA', 'NA', 'NA', 'NA', f_one, acc]], \n",
    "                            columns=list(knn_df.columns))\n",
    "        \n",
    "        knn_df = knn_df.append(temp)\n",
    "    \n",
    "    # get the end time\n",
    "    print(\"End Time:\", datetime.datetime.now().time())\n",
    "    \n",
    "    # return the df\n",
    "    return knn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the Random Forest function\n",
    "\n",
    "def random_forest(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    # get the start time\n",
    "    print(\"Start Time:\", datetime.datetime.now().time())\n",
    "    \n",
    "    # create a list for the estimator parameters\n",
    "    estimators_list = [100, 500]    \n",
    "    \n",
    "    # Create a dataframe of the excel template\n",
    "    rf_df = pd.DataFrame(columns=['Your Name',\n",
    "                               'Random State',\n",
    "                               'Algorithm',\n",
    "                               'k-Neighbors',\n",
    "                               'n_estimators',\n",
    "                               'learning_rate',\n",
    "                               'subsample',\n",
    "                               'max_depth',\n",
    "                               'Best F1 Score',\n",
    "                               'Best Accuracy Score'])\n",
    "\n",
    "    # loop through all of the estimator parameters\n",
    "    for i in estimators_list:\n",
    "    \n",
    "        # Create the classifier based on i estimators and fit it to the data along with correct labels\n",
    "        clf = RandomForestClassifier(n_estimators=i, min_samples_split=5, min_samples_leaf=5, random_state=20)\n",
    "        clf.fit(X_train, y_train)\n",
    "    \n",
    "        # create a prediction based on test set\n",
    "        y_pred = clf.predict(X_test)\n",
    "        \n",
    "        # find the F1 score (macro does not take into account class imbalances -> doesn't appear to be \n",
    "        # any in this data)\n",
    "        f_one = f1_score(y_test , y_pred, average='macro')\n",
    "        \n",
    "        # find the accuracy score\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "        temp = pd.DataFrame([['Name', 20, 'RF', 'NA', i, 'NA', 'NA', 'NA', f_one, acc]], \n",
    "                            columns=list(rf_df.columns))\n",
    "        \n",
    "        rf_df = rf_df.append(temp)\n",
    "    \n",
    "    # get the end time\n",
    "    print(\"End Time:\", datetime.datetime.now().time())\n",
    "    \n",
    "    # return the df\n",
    "    return rf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the Gradient Boosting function\n",
    "\n",
    "def gradient_boosting(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    # get the start time\n",
    "    print(\"Start Time:\", datetime.datetime.now().time())\n",
    "    \n",
    "    # create a list for the learning rate parameters\n",
    "    learning_rate_list = [0.1, 0.01, 0.001]   \n",
    "    \n",
    "    # create a list for the estimator parameters\n",
    "    estimators_list = [100, 500] \n",
    "    \n",
    "    # create a list for the subsample parameters\n",
    "    subsample_list = [0.6, 0.8, 1]\n",
    "    \n",
    "    # Create a dataframe of the excel template\n",
    "    gb_df = pd.DataFrame(columns=['Your Name',\n",
    "                               'Random State',\n",
    "                               'Algorithm',\n",
    "                               'k-Neighbors',\n",
    "                               'n_estimators',\n",
    "                               'learning_rate',\n",
    "                               'subsample',\n",
    "                               'max_depth',\n",
    "                               'Best F1 Score',\n",
    "                               'Best Accuracy Score'])\n",
    "    \n",
    "    # loop through all of the three parameters using a nested for loop system to get every combination\n",
    "    for i in learning_rate_list:\n",
    "        for j in estimators_list:\n",
    "            for k in subsample_list:\n",
    "                \n",
    "                # Create the classifier based on the three parameters from the loops\n",
    "                clf = GradientBoostingClassifier(learning_rate=i, n_estimators=j, subsample=k, min_samples_split=5, \n",
    "                                                 min_samples_leaf=5, random_state=20)\n",
    "                clf.fit(X_train, y_train)\n",
    "\n",
    "                # create a prediction based on test set\n",
    "                y_pred = clf.predict(X_test)\n",
    "    \n",
    "                # find the F1 score (macro does not take into account class imbalances -> doesn't appear to be \n",
    "                # any in this data)\n",
    "                f_one = f1_score(y_test , y_pred, average='macro')\n",
    "        \n",
    "                # find the accuracy score\n",
    "                acc = accuracy_score(y_test, y_pred)\n",
    "                \n",
    "                temp = pd.DataFrame([['Name', 20, 'GB', 'NA', j, i, k, 'NA', f_one, acc]], \n",
    "                            columns=list(gb_df.columns))\n",
    "        \n",
    "                gb_df = gb_df.append(temp)\n",
    "    \n",
    "    # get the end time\n",
    "    print(\"End Time:\", datetime.datetime.now().time())\n",
    "    \n",
    "    # return the df\n",
    "    return gb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the XGBoost function\n",
    "\n",
    "def xgboost(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    # get the start time\n",
    "    print(\"Start Time:\", datetime.datetime.now().time())\n",
    "    \n",
    "    # create a list for the learning rate parameters\n",
    "    learning_rate_list = [0.1, 0.01, 0.001]   \n",
    "    \n",
    "    # create a list for the estimator parameters\n",
    "    estimators_list = [100, 500] \n",
    "    \n",
    "    # create a list for the subsample parameters\n",
    "    subsample_list = [0.6, 0.8, 1]\n",
    "    \n",
    "    # create a list for the max depth parameters\n",
    "    max_depth_list = [5, 7, 9]\n",
    "    \n",
    "    # Create a dataframe of the excel template\n",
    "    xgb_df = pd.DataFrame(columns=['Your Name',\n",
    "                               'Random State',\n",
    "                               'Algorithm',\n",
    "                               'k-Neighbors',\n",
    "                               'n_estimators',\n",
    "                               'learning_rate',\n",
    "                               'subsample',\n",
    "                               'max_depth',\n",
    "                               'Best F1 Score',\n",
    "                               'Best Accuracy Score'])\n",
    "    \n",
    "    # loop through all of the three parameters using a nested for loop system to get every combination\n",
    "    for i in learning_rate_list:\n",
    "        for j in estimators_list:\n",
    "            for k in subsample_list:\n",
    "                for m in max_depth_list:\n",
    "                \n",
    "                    # Create the classifier based on the three parameters from the loops\n",
    "                    clf = XGBClassifier(learning_rate=i, n_estimators=j, subsample=k, max_depth=m, random_state=20)\n",
    "                    clf.fit(X_train, y_train)\n",
    "\n",
    "                    # create a prediction based on test set\n",
    "                    y_pred = clf.predict(X_test)\n",
    "    \n",
    " \n",
    "                    # find the F1 score (macro does not take into account class imbalances -> doesn't appear to be \n",
    "                    # any in this data)\n",
    "                    f_one = f1_score(y_test , y_pred, average='macro')\n",
    "        \n",
    "                    # find the accuracy score\n",
    "                    acc = accuracy_score(y_test, y_pred)\n",
    "                \n",
    "                    temp = pd.DataFrame([['Name', 20, 'XGBoost', 'NA', j, i, k, m, f_one, acc]], \n",
    "                                columns=list(xgb_df.columns))\n",
    "        \n",
    "                    xgb_df = xgb_df.append(temp)\n",
    "    \n",
    "    # get the end time\n",
    "    print(\"End Time:\", datetime.datetime.now().time())\n",
    "    \n",
    "    # return the df\n",
    "    return xgb_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create Test/Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the test/train split for pca columns\n",
    "\n",
    "# X = all pca columns, Y = rating\n",
    "X = df_dr.iloc[:,23:-1]\n",
    "Y = df_dr[\"rating_cat\"]\n",
    "\n",
    "\n",
    "X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X, Y, test_size=0.2, random_state=20)\n",
    "\n",
    "######################################################################################################################\n",
    "\n",
    "# create the test/train split for sparse pca columns\n",
    "# X = all sparse pca columns, Y = rating (from before)\n",
    "X = df_dr.iloc[:,3:23]\n",
    "\n",
    "# perform the split, using 20% for test data\n",
    "X_train_s_pca, X_test_s_pca, y_train_s_pca, y_test_s_pca = train_test_split(X, Y, test_size=0.2, random_state=20)\n",
    "\n",
    "######################################################################################################################\n",
    "\n",
    "# create the test/train split for t-SNE columns\n",
    "\n",
    "# X = all t-SNE columns, Y = rating (from before)\n",
    "X = df_dr.iloc[:,0:3]\n",
    "\n",
    "# perform the split, using 20% for test data\n",
    "X_train_tsne, X_test_tsne, y_train_tsne, y_test_tsne = train_test_split(X, Y, test_size=0.2, random_state=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the knn function for the t-SNE data\n",
    "df_knn_tsne = knn(X_train_tsne, X_test_tsne, y_train_tsne, y_test_tsne)\n",
    "print(df_knn_tsne)\n",
    "print(\"--------------------------------------------------------------------------------------\")\n",
    "\n",
    "######################################################################################################################\n",
    "\n",
    "# run the knn function for the Sparse PCA data\n",
    "df_knn_s_pca = knn(X_train_s_pca, X_test_s_pca, y_train_s_pca, y_test_s_pca)\n",
    "print(df_knn_s_pca)\n",
    "print(\"--------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "######################################################################################################################\n",
    "\n",
    "# run the knn function for the PCA data\n",
    "df_knn_pca = knn(X_train_pca, X_test_pca, y_train_pca, y_test_pca)\n",
    "print(df_knn_pca)\n",
    "print(\"--------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the Random Forest function for the t-SNE data\n",
    "df_rf_tsne = random_forest(X_train_tsne, X_test_tsne, y_train_tsne, y_test_tsne)\n",
    "print(df_rf_tsne)\n",
    "print(\"--------------------------------------------------------------------------------------\")\n",
    "\n",
    "######################################################################################################################\n",
    "\n",
    "# run the Random Forest function for the Sparse PCA data\n",
    "df_rf_s_pca = random_forest(X_train_s_pca, X_test_s_pca, y_train_s_pca, y_test_s_pca)\n",
    "print(df_rf_s_pca)\n",
    "print(\"--------------------------------------------------------------------------------------\")\n",
    "\n",
    "######################################################################################################################\n",
    "\n",
    "# run the Random Forest function for the PCA data\n",
    "df_rf_pca = random_forest(X_train_pca, X_test_pca, y_train_pca, y_test_pca)\n",
    "print(df_rf_pca)\n",
    "print(\"--------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the Gradient Boosting function for the t-SNE data\n",
    "df_gb_tsne = gradient_boosting(X_train_tsne, X_test_tsne, y_train_tsne, y_test_tsne)\n",
    "print(df_gb_tsne)\n",
    "print(\"--------------------------------------------------------------------------------------\")\n",
    "\n",
    "######################################################################################################################\n",
    "\n",
    "# run the Gradient Boosting function for the Sparse PCA data\n",
    "df_gb_s_pca = gradient_boosting(X_train_s_pca, X_test_s_pca, y_train_s_pca, y_test_s_pca)\n",
    "print(df_gb_s_pca)\n",
    "print(\"--------------------------------------------------------------------------------------\")\n",
    "\n",
    "######################################################################################################################\n",
    "\n",
    "# run the Gradient Boosting function for the PCA data\n",
    "df_gb_pca = gradient_boosting(X_train_pca, X_test_pca, y_train_pca, y_test_pca)\n",
    "print(df_gb_pca)\n",
    "print(\"--------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the XGBoost function for the t-SNE data\n",
    "df_xgb_tsne = xgboost(X_train_tsne, X_test_tsne, y_train_tsne, y_test_tsne)\n",
    "print(df_xgb_tsne)\n",
    "print(\"--------------------------------------------------------------------------------------\")\n",
    "\n",
    "######################################################################################################################\n",
    "\n",
    "# run the XGBoost function for the Sparse PCA data\n",
    "df_xgb_s_pca = xgboost(X_train_s_pca, X_test_s_pca, y_train_s_pca, y_test_s_pca)\n",
    "print(df_xgb_s_pca)\n",
    "print(\"--------------------------------------------------------------------------------------\")\n",
    "\n",
    "######################################################################################################################\n",
    "\n",
    "# run the XGBoost function for the PCA data\n",
    "df_xgb_pca = xgboost(X_train_pca, X_test_pca, y_train_pca, y_test_pca)\n",
    "print(df_xgb_pca)\n",
    "print(\"--------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Combine the Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all the model outputs for t-SNE\n",
    "df_tsne_final = pd.concat([df_knn_tsne, df_rf_tsne, df_gb_tsne, df_xgb_tsne])\n",
    "print(df_tsne_final)\n",
    "print(\"--------------------------------------------------------------------------------------\")\n",
    "\n",
    "######################################################################################################################\n",
    "\n",
    "# combine all the model outputs for Sparse PCA\n",
    "df_s_pca_final = pd.concat([df_knn_s_pca, df_rf_s_pca, df_gb_s_pca, df_xgb_s_pca])\n",
    "print(df_s_pca_final)\n",
    "print(\"--------------------------------------------------------------------------------------\")\n",
    "\n",
    "######################################################################################################################\n",
    "\n",
    "# combine all the model outputs for PCA\n",
    "df_pca_final = pd.concat([df_knn_pca, df_rf_pca, df_gb_pca, df_xgb_pca])\n",
    "print(df_pca_final)\n",
    "print(\"--------------------------------------------------------------------------------------\")\n",
    "\n",
    "######################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the files\n",
    "df_tsne_final.to_csv(\"Quiz2(t-SNE).csv\", index=False)\n",
    "df_s_pca_final.to_csv(\"Quiz2(SparsePCA).csv\", index=False)\n",
    "df_pca_final.to_csv(\"Quiz2(PCA).csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"#3c38a8\">Step 2.5: Using ADASYN to Expand Data, Then Classification</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Define ADASYN Model and Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the adasyn model, setting a seed for replicity and setting the strategy to resample all classes\n",
    "ada = ADASYN(random_state=20, sampling_strategy='all')\n",
    "\n",
    "# define an adasyn function:\n",
    "def ada_func(X, y):\n",
    "    X_res, y_res = ada.fit_resample(X, y)\n",
    "    \n",
    "    # As a note: attempting to run adasyn a second time to increase the data resulted in an error\n",
    "    \n",
    "    # Check the counts of each class\n",
    "    print(\"The number of 'Poor' values is:\", np.count_nonzero(y_res == 'Poor'))\n",
    "    print(\"The number of 'Good' values is:\", np.count_nonzero(y_res == 'Good'))\n",
    "    print(\"The number of 'Great' values is:\", np.count_nonzero(y_res == 'Great'))\n",
    "    \n",
    "    return X_res, y_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Run ADASYN and Split the Data by Dimension Reduction Technique "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data based on dimension reduction technique\n",
    "\n",
    "# all pca columns\n",
    "X_pca_ada = df_dr.iloc[:,23:-1]\n",
    "\n",
    "# all sparse pca columns\n",
    "X_s_pca_ada = df_dr.iloc[:,3:23]\n",
    "\n",
    "# all t-SNE columns\n",
    "X_tsne_ada = df_dr.iloc[:,0:3]\n",
    "\n",
    "y_ada = df_dr.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run the adasyn function to generate feature spaces for all three dimension reduction methods\n",
    "print(\"t-SNE\")\n",
    "X_res_tsne, y_res_tsne = ada_func(X_tsne_ada, y_ada)\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "print(\"Sparse PCA\")\n",
    "X_res_s_pca, y_res_s_pca = ada_func(X_s_pca_ada, y_ada)\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "print(\"PCA\")\n",
    "X_res_pca, y_res_pca = ada_func(X_pca_ada, y_ada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test/Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the test/train split for t-SNE columns, using 20% of the data as test\n",
    "X_train_tsne_ada, X_test_tsne_ada, y_train_tsne_ada, y_test_tsne_ada = train_test_split(X_res_tsne, \n",
    "                                                                                    y_res_tsne,\n",
    "                                                                                    test_size=0.2, \n",
    "                                                                                    random_state=20)\n",
    "\n",
    "######################################################################################################################\n",
    "\n",
    "# create the test/train split for sparse pca columns, using 20% of the data as test\n",
    "X_train_s_pca_ada, X_test_s_pca_ada, y_train_s_pca_ada, y_test_s_pca_ada = train_test_split(X_res_s_pca, \n",
    "                                                                                            y_res_s_pca, \n",
    "                                                                                            test_size=0.2, \n",
    "                                                                                            random_state=20)\n",
    "\n",
    "######################################################################################################################\n",
    "\n",
    "# create the test/train split for pca columns, using 20% of the data as test\n",
    "X_train_pca_ada, X_test_pca_ada, y_train_pca_ada, y_test_pca_ada = train_test_split(X_res_pca, \n",
    "                                                                                    y_res_pca,\n",
    "                                                                                    test_size=0.2, \n",
    "                                                                                    random_state=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the knn function for the t-SNE data\n",
    "df_knn_tsne_ada = knn(X_train_tsne_ada, X_test_tsne_ada, y_train_tsne_ada, y_test_tsne_ada)\n",
    "print(df_knn_tsne_ada)\n",
    "print(\"--------------------------------------------------------------------------------------\")\n",
    "\n",
    "######################################################################################################################\n",
    "\n",
    "# run the knn function for the Sparse PCA data\n",
    "df_knn_s_pca_ada = knn(X_train_s_pca_ada, X_test_s_pca_ada, y_train_s_pca_ada, y_test_s_pca_ada)\n",
    "print(df_knn_s_pca_ada)\n",
    "print(\"--------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "######################################################################################################################\n",
    "\n",
    "# run the knn function for the PCA data\n",
    "df_knn_pca_ada = knn(X_train_pca_ada, X_test_pca_ada, y_train_pca_ada, y_test_pca_ada)\n",
    "print(df_knn_pca_ada)\n",
    "print(\"--------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the Random Forest function for the t-SNE data\n",
    "df_rf_tsne_ada = random_forest(X_train_tsne_ada, X_test_tsne_ada, y_train_tsne_ada, y_test_tsne_ada)\n",
    "print(df_rf_tsne_ada)\n",
    "print(\"--------------------------------------------------------------------------------------\")\n",
    "\n",
    "######################################################################################################################\n",
    "\n",
    "# run the Random Forest function for the Sparse PCA data\n",
    "df_rf_s_pca_ada = random_forest(X_train_s_pca_ada, X_test_s_pca_ada, y_train_s_pca_ada, y_test_s_pca_ada)\n",
    "print(df_rf_s_pca_ada)\n",
    "print(\"--------------------------------------------------------------------------------------\")\n",
    "\n",
    "######################################################################################################################\n",
    "\n",
    "# run the Random Forest function for the PCA data\n",
    "df_rf_pca_ada = random_forest(X_train_pca_ada, X_test_pca_ada, y_train_pca_ada, y_test_pca_ada)\n",
    "print(df_rf_pca_ada)\n",
    "print(\"--------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the Gradient Boosting function for the t-SNE data\n",
    "df_gb_tsne_ada = gradient_boosting(X_train_tsne_ada, X_test_tsne_ada, y_train_tsne_ada, y_test_tsne_ada)\n",
    "print(df_gb_tsne_ada)\n",
    "print(\"--------------------------------------------------------------------------------------\")\n",
    "\n",
    "######################################################################################################################\n",
    "\n",
    "# run the Gradient Boosting function for the Sparse PCA data\n",
    "df_gb_s_pca_ada = gradient_boosting(X_train_s_pca_ada, X_test_s_pca_ada, y_train_s_pca_ada, y_test_s_pca_ada)\n",
    "print(df_gb_s_pca_ada)\n",
    "print(\"--------------------------------------------------------------------------------------\")\n",
    "\n",
    "######################################################################################################################\n",
    "\n",
    "# run the Gradient Boosting function for the PCA data\n",
    "df_gb_pca_ada = gradient_boosting(X_train_pca_ada, X_test_pca_ada, y_train_pca_ada, y_test_pca_ada)\n",
    "print(df_gb_pca_ada)\n",
    "print(\"--------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the XGBoost function for the t-SNE data\n",
    "df_xgb_tsne_ada = xgboost(X_train_tsne_ada, X_test_tsne_ada, y_train_tsne_ada, y_test_tsne_ada)\n",
    "print(df_xgb_tsne_ada)\n",
    "print(\"--------------------------------------------------------------------------------------\")\n",
    "\n",
    "######################################################################################################################\n",
    "\n",
    "# run the XGBoost function for the Sparse PCA data\n",
    "df_xgb_s_pca_ada = xgboost(X_train_s_pca_ada, X_test_s_pca_ada, y_train_s_pca_ada, y_test_s_pca_ada)\n",
    "print(df_xgb_s_pca_ada)\n",
    "print(\"--------------------------------------------------------------------------------------\")\n",
    "\n",
    "######################################################################################################################\n",
    "\n",
    "# run the XGBoost function for the PCA data\n",
    "df_xgb_pca_ada = xgboost(X_train_pca_ada, X_test_pca_ada, y_train_pca_ada, y_test_pca_ada)\n",
    "print(df_xgb_pca_ada)\n",
    "print(\"--------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combine the Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all the model outputs for t-SNE\n",
    "df_tsne_final_ada = pd.concat([df_knn_tsne_ada, df_rf_tsne_ada, df_gb_tsne_ada, df_xgb_tsne_ada])\n",
    "print(df_tsne_final_ada)\n",
    "print(\"--------------------------------------------------------------------------------------\")\n",
    "\n",
    "######################################################################################################################\n",
    "\n",
    "# combine all the model outputs for Sparse PCA\n",
    "df_s_pca_final_ada = pd.concat([df_knn_s_pca_ada, df_rf_s_pca_ada, df_gb_s_pca_ada, df_xgb_s_pca_ada])\n",
    "print(df_s_pca_final_ada)\n",
    "print(\"--------------------------------------------------------------------------------------\")\n",
    "\n",
    "######################################################################################################################\n",
    "\n",
    "# combine all the model outputs for PCA\n",
    "df_pca_final_ada = pd.concat([df_knn_pca_ada, df_rf_pca_ada, df_gb_pca_ada, df_xgb_pca_ada])\n",
    "print(df_pca_final_ada)\n",
    "print(\"--------------------------------------------------------------------------------------\")\n",
    "\n",
    "######################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the files\n",
    "df_tsne_final_ada.to_csv(\"Quiz2(t-SNE)-WithADASYN.csv\", index=False)\n",
    "df_s_pca_final_ada.to_csv(\"Quiz2(SparsePCA)-WithADASYN.csv\", index=False)\n",
    "df_pca_final_ada.to_csv(\"Quiz2(PCA)-WithADASYN.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"#3c38a8\">Extra Examination of Data</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Using Different Number of Columns for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the Sparse PCA columns\n",
    "\n",
    "# initialize a blank df\n",
    "knn_range_df_s_pca = pd.DataFrame()\n",
    "\n",
    "# set a count to start with the first two columns\n",
    "count = 2\n",
    "\n",
    "# loop through to get the first two then all columns\n",
    "for i in range(4, 23):\n",
    "    \n",
    "    # specify the range of x features based on the loop; keep y the same\n",
    "    X = df_dr.iloc[:,3:i]\n",
    "    Y = df_dr[\"rating_cat\"]\n",
    "\n",
    "    # perform the split, using 20% for test data\n",
    "    X_train_s_pca, X_test_s_pca, y_train_s_pca, y_test_s_pca = train_test_split(X, Y, test_size=0.2, random_state=20)\n",
    "    \n",
    "    # run the knn function for the Sparse PCA data\n",
    "    df_knn_s_pca = knn(X_train_s_pca, X_test_s_pca, y_train_s_pca, y_test_s_pca)\n",
    "    \n",
    "    # definfe names for the accuracy and F1 score using the number of PCs in the loop\n",
    "    acc_name = str(count) + \"_PCs_Accuracy\"\n",
    "    f1_name = str(count) + \"_PCs_Accuracy\"\n",
    "    \n",
    "    # create columns in the df initialized earlier that correspond to the accuracys and F1 scores calculated\n",
    "    knn_range_df_s_pca[acc_name] = df_knn_s_pca[\"Best Accuracy Score\"]\n",
    "    knn_range_df_s_pca[f1_name] = df_knn_s_pca[\"Best F1 Score\"]\n",
    "    \n",
    "    # add one to the count at the end of the loop\n",
    "    count += 1\n",
    "    \n",
    "knn_range_df_s_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the PCA columns\n",
    "\n",
    "# initialize a blank df\n",
    "knn_range_df_pca = pd.DataFrame()\n",
    "\n",
    "# set a count to start with the first two columns\n",
    "count = 2\n",
    "\n",
    "\n",
    "\n",
    "# loop through to get the first two then all columns\n",
    "for i in range(24, len(df_dr.columns)):\n",
    "    \n",
    "    # specify the range of x features based on the loop; keep y the same\n",
    "    X = df_dr.iloc[:,23:i]\n",
    "    Y = df_dr[\"rating_cat\"]\n",
    "\n",
    "    # perform the split, using 20% for test data\n",
    "    X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X, Y, test_size=0.2, random_state=20)\n",
    "    \n",
    "    # run the knn function for the Sparse PCA data\n",
    "    df_knn_pca = knn(X_train_pca, X_test_pca, y_train_pca, y_test_pca)\n",
    "    \n",
    "    # definfe names for the accuracy and F1 score using the number of PCs in the loop\n",
    "    acc_name = str(count) + \"_PCs_Accuracy\"\n",
    "    f1_name = str(count) + \"_PCs_Accuracy\"\n",
    "    \n",
    "    # create columns in the df initialized earlier that correspond to the accuracys and F1 scores calculated\n",
    "    knn_range_df_pca[acc_name] = df_knn_pca[\"Best Accuracy Score\"]\n",
    "    knn_range_df_pca[f1_name] = df_knn_pca[\"Best F1 Score\"]\n",
    "    \n",
    "    # add one to the count at the end of the loop\n",
    "    count += 1\n",
    "    \n",
    "knn_range_df_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not appear that the number of columns makes any difference in the scoring of the Sparse PCA and PCA data sets. However, this check was done in a sequential manner; perhaps using a random assortment of a random number of PCs would generate some higher results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Remove Test/Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_pca = all pca columns\n",
    "X_pca = df_dr.iloc[:,23:-1]\n",
    "\n",
    "# X_s_pca = all sparse pca columns\n",
    "X_s_pca = df_dr.iloc[:,3:23]\n",
    "\n",
    "# X_tsne = all t-SNE columns\n",
    "X_tsne = df_dr.iloc[:,0:3]\n",
    "\n",
    "# Y_all = the response values\n",
    "Y_all = df_dr[\"rating_cat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the knn function for the t-SNE data\n",
    "df_knn_tsne_all = knn(X_tsne, X_tsne, Y_all, Y_all)\n",
    "print(df_knn_tsne_all)\n",
    "print(\"--------------------------------------------------------------------------------------\")\n",
    "\n",
    "######################################################################################################################\n",
    "\n",
    "# run the knn function for the Sparse PCA data\n",
    "df_knn_s_pca_all = knn(X_s_pca, X_s_pca, Y_all, Y_all)\n",
    "print(df_knn_s_pca_all)\n",
    "print(\"--------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "######################################################################################################################\n",
    "\n",
    "# run the knn function for the PCA data\n",
    "df_knn_pca_all = knn(X_pca, X_pca, Y_all, Y_all)\n",
    "print(df_knn_pca_all)\n",
    "print(\"--------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the Random Forest function for the t-SNE data\n",
    "df_rf_tsne_all = random_forest(X_pca, X_pca, Y_all, Y_all)\n",
    "print(df_rf_tsne_all)\n",
    "print(\"--------------------------------------------------------------------------------------\")\n",
    "\n",
    "######################################################################################################################\n",
    "\n",
    "# run the Random Forest function for the Sparse PCA data\n",
    "df_rf_s_pca_all = random_forest(X_pca, X_pca, Y_all, Y_all)\n",
    "print(df_rf_s_pca_all)\n",
    "print(\"--------------------------------------------------------------------------------------\")\n",
    "\n",
    "######################################################################################################################\n",
    "\n",
    "# run the Random Forest function for the PCA data\n",
    "df_rf_pca_all = random_forest(X_pca, X_pca, Y_all, Y_all)\n",
    "print(df_rf_pca_all)\n",
    "print(\"--------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the Gradient Boosting function for the t-SNE data\n",
    "df_gb_tsne_all = gradient_boosting(X_pca, X_pca, Y_all, Y_all)\n",
    "print(df_gb_tsne_all)\n",
    "print(\"--------------------------------------------------------------------------------------\")\n",
    "\n",
    "######################################################################################################################\n",
    "\n",
    "# run the Gradient Boosting function for the Sparse PCA data\n",
    "df_gb_s_pca_all = gradient_boosting(X_pca, X_pca, Y_all, Y_all)\n",
    "print(df_gb_s_pca_all)\n",
    "print(\"--------------------------------------------------------------------------------------\")\n",
    "\n",
    "######################################################################################################################\n",
    "\n",
    "# run the Gradient Boosting function for the PCA data\n",
    "df_gb_pca_all = gradient_boosting(X_pca, X_pca, Y_all, Y_all)\n",
    "print(df_gb_pca_all)\n",
    "print(\"--------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the XGBoost function for the t-SNE data\n",
    "df_xgb_tsne_all = xgboost(X_pca, X_pca, Y_all, Y_all)\n",
    "print(df_xgb_tsne_all)\n",
    "print(\"--------------------------------------------------------------------------------------\")\n",
    "\n",
    "######################################################################################################################\n",
    "\n",
    "# run the XGBoost function for the Sparse PCA data\n",
    "df_xgb_s_pca_all = xgboost(X_pca, X_pca, Y_all, Y_all)\n",
    "print(df_xgb_s_pca_all)\n",
    "print(\"--------------------------------------------------------------------------------------\")\n",
    "\n",
    "######################################################################################################################\n",
    "\n",
    "# run the XGBoost function for the PCA data\n",
    "df_xgb_pca_all = xgboost(X_pca, X_pca, Y_all, Y_all)\n",
    "print(df_xgb_pca_all)\n",
    "print(\"--------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conclusion I can draw from this experiment is that using all of the data (no test/train split) results in better scores for F1 and accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
